{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "import pandas as pd\n",
    "import os \n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import traceback\n",
    "import warnings\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from deep_translator import GoogleTranslator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder with the name of data\n",
    "os.makedirs('data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Stage 0: Data Preprocessing to Extract the Python Code**\n",
    "1. Downloaded the Dataset: Downloaded the \"jtatman/python-code-dataset-500k\" from Hugging Face.\n",
    "2. Data Cleansing: Cleansed the data and extracted just the Python code using regex where the Python code was enclosed in between <pythoncode>.\n",
    "3. Store Extracted Code: Stored all the Python code in python_code_dataset.csv file with the column name English_code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nfor i in range(10):  # First digit\\n    for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\ndef count_distinct_states(matrix):\\n    coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\ndef remove_spaces_and_punctuation(s):\\n    r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nimport math\\n\\ndef is_prime(n):\\n    # Check...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nclass String:\\n    def __init__(self, string...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        English_code\n",
       "0  \\nfor i in range(10):  # First digit\\n    for ...\n",
       "1  \\ndef count_distinct_states(matrix):\\n    coun...\n",
       "2  \\ndef remove_spaces_and_punctuation(s):\\n    r...\n",
       "3  \\nimport math\\n\\ndef is_prime(n):\\n    # Check...\n",
       "4  \\nclass String:\\n    def __init__(self, string..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from Hugging Face Datasets and store the output column after extracting the python code into a csv file \n",
    "ds = load_dataset(\"jtatman/python-code-dataset-500k\")\n",
    "ds = pd.DataFrame(ds['train'])\n",
    "ds = ds.drop(['instruction', 'system'], axis=1)\n",
    "ds['English_code'] = ds['output'].apply(lambda x: re.search(r'```python(.*?)```', x, re.DOTALL).group(1) if re.search(r'```python(.*?)```', x, re.DOTALL) else None)\n",
    "ds = ds.drop(['output'], axis=1)\n",
    "ds = ds.dropna()\n",
    "ds.to_csv('data/python_code_dataset.csv', index=False)\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>67063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>25549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>\\ndef factorial(n):\\n    if n == 0:\\n        r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English_code\n",
       "count                                               67063\n",
       "unique                                              25549\n",
       "top     \\ndef factorial(n):\\n    if n == 0:\\n        r...\n",
       "freq                                                   41"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stage 1: Initial Translation**\n",
    "\n",
    "1. **Initial Translation:** Used the ```python_code_dataset.csv``` for the initial translation using Google Translator.\n",
    "\n",
    "2. **Keyword Dictionary Creation:** Created a keyword dictionary using the curated dataset by Joshua Otten, which includes files like ```FrenchKey.txt```, ```SpanishKey.txt```, ```KurdishKey.txt```, ```BengaliKey.txt```, ```MandarinKey.txt```, ```GreekKey.txt```, ```EnglishKey.txt```, and ```HindiKey.txt```.\n",
    "\n",
    "3. **Mapping Keywords:** Mapped keywords from ```EnglishKey.txt``` to```HindiKey.txt``` for partial translation.\n",
    "\n",
    "4. **Partial Translation:** Parsed the code one by one, replacing English keywords with their Hindi counterparts from the dictionary.\n",
    "\n",
    "5. **Google Translator:** Provided the partially translated code to Google Translator, ensuring:\n",
    "    - Comments and strings were translated as a whole.\n",
    "    - Variables separated by underscores were split, translated, and rejoined to maintain consistency.\n",
    "\n",
    "6. **Final Outcome:** The final outcome was named ```Hindi_code_version_1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Found 50 unprocessed items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating code: 100%|██████████| 50/50 [05:00<00:00,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed! Results saved to: data/google_code_translations.csv\n",
      "\n",
      "Processing completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self, max_rows=None):\n",
    "        self.input_path = 'data/python_code_dataset.csv'\n",
    "        self.output_path = 'data/google_code_translations.csv'\n",
    "        self.checkpoint_path = 'data/translation_checkpoint.json'\n",
    "        self.keywords_path = './Joshua_Keywords.csv'\n",
    "        self.batch_size = 5\n",
    "        self.source_lang = 'en'\n",
    "        self.target_lang = 'hi'\n",
    "        self.sleep_time = 0.2\n",
    "        self.max_retries = 3\n",
    "        self.max_rows = max_rows\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"Manages saving and loading of translation progress\"\"\"\n",
    "    def __init__(self, checkpoint_path: str):\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.processed_indices = set()\n",
    "        self._load_checkpoint()\n",
    "\n",
    "    def _load_checkpoint(self) -> None:\n",
    "        \"\"\"Load existing checkpoint if available\"\"\"\n",
    "        if os.path.exists(self.checkpoint_path):\n",
    "            try:\n",
    "                with open(self.checkpoint_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                self.processed_indices = set(data.get('processed_indices', []))\n",
    "                print(f\"Loaded checkpoint with {len(self.processed_indices)} processed items\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading checkpoint: {str(e)}\")\n",
    "                self.processed_indices = set()\n",
    "\n",
    "    def save_checkpoint(self) -> None:\n",
    "        \"\"\"Save current progress to checkpoint file\"\"\"\n",
    "        try:\n",
    "            data = {'processed_indices': list(self.processed_indices)}\n",
    "            with open(self.checkpoint_path, 'w') as f:\n",
    "                json.dump(data, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving checkpoint: {str(e)}\")\n",
    "\n",
    "    def mark_processed(self, index: int) -> None:\n",
    "        \"\"\"Mark an item as processed and save checkpoint\"\"\"\n",
    "        self.processed_indices.add(index)\n",
    "        self.save_checkpoint()\n",
    "\n",
    "    def is_processed(self, index: int) -> bool:\n",
    "        \"\"\"Check if an item has been processed\"\"\"\n",
    "        return index in self.processed_indices\n",
    "\n",
    "    def get_unprocessed_indices(self, total_items: int) -> List[int]:\n",
    "        \"\"\"Get list of indices that haven't been processed yet\"\"\"\n",
    "        return [i for i in range(total_items) if not self.is_processed(i)]\n",
    "\n",
    "class KeywordManager:\n",
    "    \"\"\"Manages programming keyword translations\"\"\"\n",
    "    def __init__(self, keywords_path: str):\n",
    "        self.keywords_path = keywords_path\n",
    "        self.keywords = self._load_keywords()\n",
    "        self._add_special_cases()\n",
    "\n",
    "    def _load_keywords(self) -> Dict[str, str]:\n",
    "        \"\"\"Load keyword translations from file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(self.keywords_path)\n",
    "            # Drop non-Hindi translations\n",
    "            columns_to_drop = [\n",
    "                'FrenchKey.txt', 'SpanishKey.txt', 'KurdishKey.txt',\n",
    "                'BengaliKey.txt', 'MandarinKey.txt', 'GreekKey.txt'\n",
    "            ]\n",
    "            df.drop(columns=columns_to_drop, inplace=True)\n",
    "            df.dropna(inplace=True)\n",
    "            return {row['EnglishKey.txt']: row['HindiKey.txt'] for _, row in df.iterrows()}\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading keywords: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def _add_special_cases(self) -> None:\n",
    "        \"\"\"Add special case translations\"\"\"\n",
    "        special_cases = {\n",
    "            'i': 'ई',\n",
    "            'j': 'जे',\n",
    "            'k': 'के'\n",
    "        }\n",
    "        self.keywords.update(special_cases)\n",
    "\n",
    "    def get_translation(self, word: str) -> Optional[str]:\n",
    "        \"\"\"Get translation for a keyword if available\"\"\"\n",
    "        return self.keywords.get(word)\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    \"\"\"Dataset for code translation\"\"\"\n",
    "    def __init__(self, codes: List[str], indices: List[int]):\n",
    "        self.codes = codes\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.codes)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, any]:\n",
    "        return {\n",
    "            'index': self.indices[idx],\n",
    "            'code': self.codes[idx]\n",
    "        }\n",
    "\n",
    "def custom_collate(batch: List[Dict]) -> Dict[str, List]:\n",
    "    \"\"\"Custom collate function for DataLoader\"\"\"\n",
    "    return {\n",
    "        'indices': [item['index'] for item in batch],\n",
    "        'codes': [item['code'] for item in batch]\n",
    "    }\n",
    "\n",
    "class CodeTranslator:\n",
    "    def __init__(self, config: Config, keyword_manager: KeywordManager):\n",
    "        self.config = config\n",
    "        self.keyword_manager = keyword_manager\n",
    "        self.translator = GoogleTranslator(\n",
    "            source=config.source_lang,\n",
    "            target=config.target_lang\n",
    "        )\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "    def process_compound_word(self, word: str) -> str:\n",
    "        \"\"\"Handle translation of compound words with underscores\"\"\"\n",
    "        if '_' in word:\n",
    "            parts = word.split('_')\n",
    "            translated_parts = []\n",
    "            for part in parts:\n",
    "                translated = self.safe_translate(part)\n",
    "                # If translation contains space, replace with underscore\n",
    "                translated = translated.replace(' ', '_') if translated else part\n",
    "                translated_parts.append(translated)\n",
    "            return '_'.join(translated_parts)\n",
    "        return word\n",
    "\n",
    "    def translate_token(self, token: str) -> str:\n",
    "        if token.isspace():\n",
    "            return token\n",
    "        elif '_' in token:\n",
    "            parts = token.split('_')\n",
    "            translated_parts = []\n",
    "            for part in parts:\n",
    "                if part:\n",
    "                    keyword_trans = self.keyword_manager.get_translation(part)\n",
    "                    if keyword_trans:\n",
    "                        translated_parts.append(keyword_trans)\n",
    "                    else:\n",
    "                        trans = self.safe_translate(part)\n",
    "                        if ' ' in trans:\n",
    "                            trans = trans.replace(' ', '_')\n",
    "                        translated_parts.append(trans)\n",
    "            return '_'.join(translated_parts)\n",
    "        elif token.isalpha():\n",
    "            keyword_trans = self.keyword_manager.get_translation(token)\n",
    "            if keyword_trans:\n",
    "                return keyword_trans.replace(' ', '_')\n",
    "            translation = self.safe_translate(token)\n",
    "            return translation.replace(' ', '_')\n",
    "        return token\n",
    "\n",
    "    def safe_translate(self, text: str) -> str:\n",
    "        if not text or not isinstance(text, str):\n",
    "            return text\n",
    "\n",
    "        for attempt in range(self.config.max_retries):\n",
    "            try:\n",
    "                translated = self.translator.translate(text)\n",
    "                if ' ' in translated:\n",
    "                    translated = translated.replace(' ', '_')\n",
    "                if any(c.isascii() and c.isalpha() for c in translated):\n",
    "                    translated = self.translator.translate(text.lower()).replace(' ', '_')\n",
    "                return translated\n",
    "            except Exception as e:\n",
    "                if attempt == self.config.max_retries - 1:\n",
    "                    return text\n",
    "        return text\n",
    "\n",
    "    def translate_line(self, line: str) -> str:\n",
    "        indent = len(line) - len(line.lstrip())\n",
    "        line = line.lstrip()\n",
    "\n",
    "        if not line:\n",
    "            return line\n",
    "\n",
    "        try:\n",
    "            if '#' in line:\n",
    "                code_part, comment_part = line.split('#', 1)\n",
    "                translated_comment = self.safe_translate(comment_part.strip())\n",
    "\n",
    "                if code_part:\n",
    "                    tokens = re.findall(r'[a-zA-Z_]+|\\d+|[^\\w\\s]|\\s+', code_part)\n",
    "                    translated_tokens = [self.translate_token(token) for token in tokens]\n",
    "                    translated_code = ''.join(translated_tokens)\n",
    "                    return ' ' * indent + translated_code.rstrip() + ' #' + translated_comment\n",
    "                return ' ' * indent + '#' + translated_comment\n",
    "\n",
    "            tokens = re.findall(r'[a-zA-Z_]+|\\d+|[^\\w\\s]|\\s+', line)\n",
    "            translated_tokens = [self.translate_token(token) for token in tokens]\n",
    "            return ' ' * indent + ''.join(translated_tokens)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Line translation error: {str(e)}\")\n",
    "            return line\n",
    "    def translate_code(self, code: str) -> str:\n",
    "        if not isinstance(code, str):\n",
    "            return \"\"\n",
    "\n",
    "        if '\\\\n' in code:\n",
    "            lines = code.strip(\"'\\\"\").split('\\\\n')\n",
    "            translated_lines = [self.translate_line(line.strip()) for line in lines]\n",
    "            return '\\\\n '.join(translated_lines)\n",
    "\n",
    "        lines = code.split('\\n')\n",
    "        translated_lines = [self.translate_line(line) for line in lines]\n",
    "        return '\\n'.join(translated_lines)\n",
    "\n",
    "    def process_batch(self, batch: Dict[str, List]) -> Tuple[List[int], List[str]]:\n",
    "        \"\"\"Process a batch of code samples\"\"\"\n",
    "        indices = batch['indices']\n",
    "        codes = batch['codes']\n",
    "\n",
    "        translated_batch = []\n",
    "        for code in codes:\n",
    "            if isinstance(code, torch.Tensor):\n",
    "                code = code.cpu().numpy().item()\n",
    "            translated_code = self.translate_code(code)\n",
    "            translated_batch.append(translated_code)\n",
    "\n",
    "        return indices, translated_batch\n",
    "\n",
    "class TranslationManager:\n",
    "    \"\"\"Manages the overall translation process\"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.checkpoint_manager = CheckpointManager(config.checkpoint_path)\n",
    "        self.keyword_manager = KeywordManager(config.keywords_path)\n",
    "        self.translator = CodeTranslator(config, self.keyword_manager)\n",
    "\n",
    "    def prepare_data(self) -> Tuple[pd.DataFrame, List[int]]:\n",
    "        if os.path.exists(self.config.output_path):\n",
    "            results_df = pd.read_csv(self.config.output_path)\n",
    "            input_df = pd.read_csv(self.config.input_path)\n",
    "        else:\n",
    "            input_df = pd.read_csv(self.config.input_path)\n",
    "            if self.config.max_rows:\n",
    "                input_df = input_df.head(self.config.max_rows)\n",
    "            results_df = pd.DataFrame({\n",
    "                'English_code': input_df['English_code'],\n",
    "                'Hindi_code': [None] * len(input_df)\n",
    "            })\n",
    "\n",
    "        unprocessed_indices = self.checkpoint_manager.get_unprocessed_indices(len(input_df))\n",
    "        if self.config.max_rows:\n",
    "            unprocessed_indices = unprocessed_indices[:self.config.max_rows]\n",
    "        return results_df, unprocessed_indices\n",
    "\n",
    "    def process_translations(self) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Process all translations with checkpointing\"\"\"\n",
    "        try:\n",
    "            results_df, unprocessed_indices = self.prepare_data()\n",
    "\n",
    "            if not unprocessed_indices:\n",
    "                print(\"All items have been processed!\")\n",
    "                return results_df\n",
    "\n",
    "            print(f\"Found {len(unprocessed_indices)} unprocessed items\")\n",
    "\n",
    "            # Create dataset and dataloader\n",
    "            unprocessed_codes = [\n",
    "                results_df.iloc[i]['English_code'] for i in unprocessed_indices\n",
    "            ]\n",
    "            dataset = CodeDataset(unprocessed_codes, unprocessed_indices)\n",
    "            dataloader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=self.config.batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=custom_collate\n",
    "            )\n",
    "\n",
    "            # Process batches\n",
    "            try:\n",
    "                with tqdm(total=len(unprocessed_indices), desc=\"Translating code\") as pbar:\n",
    "                    for batch in dataloader:\n",
    "                        indices, translated_codes = self.translator.process_batch(batch)\n",
    "\n",
    "                        # Update results and save progress\n",
    "                        for idx, translated_code in zip(indices, translated_codes):\n",
    "                            results_df.at[idx, 'Hindi_code'] = translated_code\n",
    "                            self.checkpoint_manager.mark_processed(idx)\n",
    "\n",
    "                        # Save intermediate results\n",
    "                        results_df.to_csv(self.config.output_path, index=False)\n",
    "                        pbar.update(len(indices))\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nProcess interrupted by user. Saving progress...\")\n",
    "                results_df.to_csv(self.config.output_path, index=False)\n",
    "                return results_df\n",
    "\n",
    "            print(f\"\\nProcessing completed! Results saved to: {self.config.output_path}\")\n",
    "            return results_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during processing: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            if 'results_df' in locals():\n",
    "                results_df.to_csv(self.config.output_path, index=False)\n",
    "                return results_df\n",
    "            return None\n",
    "\n",
    "\n",
    "\"\"\"Main entry point\"\"\"\n",
    "# Create config\n",
    "config = Config(max_rows=50)\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "Path('data').mkdir(exist_ok=True)\n",
    "\n",
    "# Clean up GPU memory if available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize and run translation manager\n",
    "manager = TranslationManager(config)\n",
    "processed_df = manager.process_translations()\n",
    "\n",
    "if processed_df is not None:\n",
    "    print(\"\\nProcessing completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stage 2: GPT Enhancement: Example-based Translation using GPT 4o Mini**\n",
    "\n",
    "1. **Base Example:** Used the ```Hindi_code_version_1``` from Stage 1 as a base example for the model.\n",
    "\n",
    "2. **Partial Translation:** Took unseen English Python code and performed partial translation using the Keyword Dictionary, similar to Stage 1, Step 4.\n",
    "\n",
    "3. **Model Input:** Provided both the example pair ```Hindi_code_version_1``` and a different partially translated code (test case) to the GPT model with a customized prompt.\n",
    "\n",
    "4. **Translation Request:** Asked the GPT model to translate the test case using the example pair as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load API Key from a text file\n",
    "with open('../API.txt', 'r') as f:\n",
    "    API_key = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, max_rows=None, example_count = 5, output_path = 'data/translations_gpt.csv'):\n",
    "        self.input_path = '../few_shot_Learning_data.csv'\n",
    "        self.output_path = output_path\n",
    "        self.checkpoint_path = 'data/checkpoint_gpt.json'\n",
    "        self.max_rows = max_rows\n",
    "        self.batch_size = 5\n",
    "        self.openai_api_key = API_key\n",
    "        self.examples_count = example_count\n",
    "        self.client = OpenAI(api_key=self.openai_api_key)\n",
    "\n",
    "class KeywordReplacer:\n",
    "    def __init__(self):\n",
    "        self.keywords = self._load_keywords()\n",
    "        \n",
    "    def _load_keywords(self):\n",
    "        df = pd.read_csv('./Joshua_Keywords.csv')\n",
    "        columns_to_drop = [\n",
    "            'FrenchKey.txt', 'SpanishKey.txt', 'KurdishKey.txt',\n",
    "            'BengaliKey.txt', 'MandarinKey.txt', 'GreekKey.txt'\n",
    "        ]\n",
    "        df.drop(columns=columns_to_drop, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        return {row['EnglishKey.txt']: row['HindiKey.txt'] for _, row in df.iterrows()}\n",
    "    \n",
    "    def replace_keywords(self, code):\n",
    "\n",
    "        # Split code into tokens while preserving structure\n",
    "        tokens = re.findall(r'[a-zA-Z_]+|\\d+|[^\\w\\s]|\\s+', code)\n",
    "        translated_tokens = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in self.keywords:\n",
    "                translated_tokens.append(self.keywords[token])\n",
    "            elif token == 'True':\n",
    "                translated_tokens.append('सत्य')\n",
    "            elif token == 'False':\n",
    "                translated_tokens.append('असत्य')\n",
    "            else:\n",
    "                translated_tokens.append(token)\n",
    "                \n",
    "        return ''.join(translated_tokens)\n",
    "\n",
    "class GPTTranslator:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.examples = self.load_examples()\n",
    "        self.keyword_replacer = KeywordReplacer()\n",
    "\n",
    "    def load_examples(self):\n",
    "        df = pd.read_csv(self.config.input_path)\n",
    "        return df.head(self.config.examples_count)[['English_code', 'Hindi_code']]\n",
    "\n",
    "    def create_prompt(self, code_to_translate):\n",
    "        examples_text = \"\"\n",
    "        for i, row in self.examples.iterrows():\n",
    "            examples_text += f\"\\n\\nExample {i+1}:\\n\"\n",
    "            examples_text += f\"English code:\\n{row['English_code']}\\n\"\n",
    "            examples_text += f\"Hindi translated code:\\n{row['Hindi_code']}\\n------------------------\\n\"\n",
    "            \n",
    "        prompt = f\"\"\"Complete the translation of this partially English Python code to completely Hindi python code:\n",
    "        - Translate variable names, function names, strings and comments to Hindi\n",
    "        - Join multi-word Hindi translations with underscores\n",
    "        - Break down compound English words separated by underscores and translate each part into sensible Hindi and join them back with underscores\n",
    "        - Preserve code structure and syntax\n",
    "        - Here are some examples of translations:\n",
    "    \n",
    "        {examples_text}\n",
    "        \n",
    "        Now translate partially translated code to completely in Hindi:\n",
    "        {code_to_translate}\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "    def translate_code(self, code):\n",
    "        # First replace known keywords\n",
    "        partially_translated = self.keyword_replacer.replace_keywords(code)\n",
    "\n",
    "        # Then use GPT to complete the translation\n",
    "        prompt = self.create_prompt(partially_translated)\n",
    "        try:\n",
    "            # print(f\"Prompt:\\n{prompt}\")\n",
    "            response = self.config.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a Expert Python code translator who understands the nuanses of language in coding and converts code from English to  Hindi code while preserving functionality. Return only the translated code without any explanation.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            translated_code = response.choices[0].message.content.strip()\n",
    "\n",
    "            # Clean up the response to extract just the code\n",
    "            if \"```python\" in translated_code:\n",
    "                translated_code = translated_code.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "            elif \"```\" in translated_code:\n",
    "                translated_code = translated_code.split(\"```\")[1].strip()\n",
    "                \n",
    "            return translated_code\n",
    "        except Exception as e:\n",
    "            print(f\"Translation error: {str(e)}\")\n",
    "            return code\n",
    "\n",
    "def run_translation(max_rows=None, example_count=5, file_path='data/translations_gpt.csv'):\n",
    "    config = Config(max_rows=max_rows, example_count=example_count, output_path=file_path)\n",
    "    translator = GPTTranslator(config)\n",
    "    \n",
    "    df = pd.read_csv(config.input_path)\n",
    "    if max_rows:\n",
    "        df = df.iloc[30:max_rows+30] # it will always start from 30th row and take 10 rows ahead of it\n",
    "    \n",
    "    results = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        translated = translator.translate_code(row['English_code'])\n",
    "        results.append({\n",
    "            'English_code': row['English_code'],\n",
    "            'Hindi_code': translated\n",
    "        })\n",
    "        \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(config.output_path, index=False)\n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:43<00:00,  4.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation results saved to: data/translations_gpt_10r_5e.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:48<00:00,  4.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation results saved to: data/translations_gpt_10r_10e.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:53<00:00,  5.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation results saved to: data/translations_gpt_10r_20e.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:02<00:00,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation results saved to: data/translations_gpt_10r_30e.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_translation_file_path = [\n",
    "    'data/translations_gpt_10r_5e.csv',\n",
    "    'data/translations_gpt_10r_10e.csv',\n",
    "    'data/translations_gpt_10r_20e.csv',\n",
    "    'data/translations_gpt_10r_30e.csv'\n",
    "]\n",
    "example_counts = [5, 10, 20, 30]\n",
    "# Usage\n",
    "'''\n",
    "max_rows is the number of rows to translate. If None, all rows will be translated.\n",
    "example_count is the number of examples to use for GPT prompt. It should be less than max_rows.\n",
    "'''\n",
    "for file_path, example_count in zip(gpt_translation_file_path, example_counts):\n",
    "    translated_df = run_translation(max_rows=10, example_count=example_count, file_path=file_path)\n",
    "    print(f\"Translation results saved to: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "\n",
    "def keyword_reverse_translation(code, reverse_keywords):\n",
    "    tokens = re.findall(r'[\\u0900-\\u097F_]+|[a-zA-Z_]+|\\d+|[^\\w\\s]|\\s+', code)\n",
    "    translated_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in reverse_keywords:\n",
    "            translated_tokens.append(reverse_keywords[token])\n",
    "        else:\n",
    "            translated_tokens.append(token)\n",
    "    \n",
    "    return ''.join(translated_tokens)\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.keyword_replacer = KeywordReplacer()\n",
    "        self.reverse_keywords = {v: k for k, v in self.keyword_replacer.keywords.items()}\n",
    "\n",
    "    def reverse_translate_code(self, hindi_code):\n",
    "        # First replace known keywords\n",
    "        partially_translated = keyword_reverse_translation(hindi_code, self.reverse_keywords)\n",
    "        \n",
    "        prompt = f\"\"\"Complete the translation of this partially translated Python code to English:\n",
    "        - The code already has Python keywords translated to English\n",
    "        - Translate remaining variable names and comments\n",
    "        - Convert Hindi compound words (with underscores) to appropriate English terms\n",
    "        - Preserve code structure and syntax\n",
    "        \n",
    "        Partially translated code:\n",
    "        {partially_translated}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.config.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a Python code translator converting Hindi code to English.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            return self.clean_code(response.choices[0].message.content.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Reverse translation error: {str(e)}\")\n",
    "            return hindi_code\n",
    "    def clean_code(self, code):\n",
    "        \"\"\"Remove markdown and normalize code\"\"\"\n",
    "        if \"```python\" in code:\n",
    "            code = code.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in code:\n",
    "            code = code.split(\"```\")[1].strip()\n",
    "        return code.strip()\n",
    "\n",
    "\n",
    "    def evaluate_translations(self, df):\n",
    "        \"\"\"Evaluate translations using round-trip and BLEU score\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for _, row in tqdm(df.iterrows(), desc=\"Evaluating translations\"):\n",
    "            original = row['English_code']\n",
    "            hindi = row['Hindi_code']\n",
    "            \n",
    "            # Round-trip translation\n",
    "            back_translated = self.reverse_translate_code(hindi)\n",
    "            \n",
    "            results.append({\n",
    "                'original': original,\n",
    "                'hindi': hindi,\n",
    "                'back_translated': back_translated,\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Usage example\n",
    "def run_evaluation( gpt_translation_file_path = '', evaluation_results_file_path = ''):\n",
    "    config = Config()\n",
    "    evaluator = TranslationEvaluator(config)\n",
    "    \n",
    "    # Load translations\n",
    "    df = pd.read_csv(gpt_translation_file_path) # change the file name to the file you want to evaluate\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.evaluate_translations(df)\n",
    "    \n",
    "    # Save results\n",
    "    results.to_csv(evaluation_results_file_path, index=False) # change the file name to save the results accordingly\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ankitkumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Evaluating translations: 10it [00:31,  3.19s/it]\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ankitkumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved to: data/backtranslation_results_10r_5e.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating translations: 10it [00:28,  2.80s/it]\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ankitkumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved to: data/backtranslation_results_10r_10e.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating translations: 10it [00:37,  3.78s/it]\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ankitkumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved to: data/backtranslation_results_10r_20e.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating translations: 10it [00:32,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved to: data/backtranslation_results_10r_30e.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run evaluation\n",
    "evaluation_results_file_path = [\n",
    "    'data/backtranslation_results_10r_5e.csv',\n",
    "    'data/backtranslation_results_10r_10e.csv',\n",
    "    'data/backtranslation_results_10r_20e.csv',\n",
    "    'data/backtranslation_results_10r_30e.csv'\n",
    "]\n",
    "\n",
    "for input_file, output_file in zip(gpt_translation_file_path, evaluation_results_file_path):\n",
    "    evaluation_results = run_evaluation(input_file, output_file)\n",
    "    print(f\"Evaluation results saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Summary for data/backtranslation_results_10r_5e.csv:\n",
      "Average BLEU Score: 0.7150\n",
      "Syntax Valid Rate: 1.0000\n",
      "Average Semantic Similarity: 0.9296\n",
      "Overall Score: 0.8815\n",
      "\n",
      "Evaluation Summary for data/backtranslation_results_10r_10e.csv:\n",
      "Average BLEU Score: 0.7035\n",
      "Syntax Valid Rate: 1.0000\n",
      "Average Semantic Similarity: 0.9370\n",
      "Overall Score: 0.8802\n",
      "\n",
      "Evaluation Summary for data/backtranslation_results_10r_20e.csv:\n",
      "Average BLEU Score: 0.6860\n",
      "Syntax Valid Rate: 0.9000\n",
      "Average Semantic Similarity: 0.8662\n",
      "Overall Score: 0.8174\n",
      "\n",
      "Evaluation Summary for data/backtranslation_results_10r_30e.csv:\n",
      "Average BLEU Score: 0.7357\n",
      "Syntax Valid Rate: 1.0000\n",
      "Average Semantic Similarity: 0.9070\n",
      "Overall Score: 0.8809\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import ast\n",
    "import tokenize\n",
    "from io import StringIO\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "import re\n",
    "\n",
    "class CodeTranslationEvaluator:\n",
    "    def __init__(self):\n",
    "        self.semantic_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "        \n",
    "    def evaluate_translation(self, original_code: str, back_translated_code: str) -> dict:\n",
    "        \"\"\"Main evaluation function that combines all metrics\"\"\"\n",
    "        results = {\n",
    "            'bleu_score': self.calculate_bleu_score(original_code, back_translated_code),\n",
    "            'syntax_validation': self.validate_syntax(back_translated_code),\n",
    "            'semantic_similarity': self.evaluate_semantic_similarity(original_code, back_translated_code)\n",
    "        }\n",
    "        \n",
    "        # Calculate overall score\n",
    "        valid_scores = [score for score in [\n",
    "            results['bleu_score'],\n",
    "            1.0 if results['syntax_validation']['is_valid'] else 0.0,\n",
    "            results['semantic_similarity']['overall_semantic_score']\n",
    "        ] if score is not None]\n",
    "        \n",
    "        results['overall_score'] = np.mean(valid_scores) if valid_scores else 0.0\n",
    "        return results\n",
    "\n",
    "    def calculate_bleu_score(self, original_code: str, back_translated_code: str) -> float:\n",
    "        \"\"\"Calculate BLEU score between original and back-translated code\"\"\"\n",
    "        def tokenize_code(code):\n",
    "            return code.replace('\\n', ' ').split()\n",
    "        \n",
    "        reference = [tokenize_code(original_code)]\n",
    "        candidate = tokenize_code(back_translated_code)\n",
    "        \n",
    "        return sentence_bleu(reference, candidate)\n",
    "\n",
    "    def validate_syntax(self, code: str) -> dict:\n",
    "        \"\"\"Check if the code is syntactically valid Python\"\"\"\n",
    "        try:\n",
    "            ast.parse(code)\n",
    "            return {\n",
    "                'is_valid': True,\n",
    "                'error': None,\n",
    "                'token_structure': self._analyze_token_structure(code)\n",
    "            }\n",
    "        except SyntaxError as e:\n",
    "            return {\n",
    "                'is_valid': False,\n",
    "                'error': str(e),\n",
    "                'token_structure': None\n",
    "            }\n",
    "\n",
    "    def _analyze_token_structure(self, code: str) -> dict:\n",
    "        \"\"\"Analyze the token structure of the code\"\"\"\n",
    "        try:\n",
    "            tokens = list(tokenize.generate_tokens(StringIO(code).readline))\n",
    "            return {\n",
    "                'total_tokens': len(tokens),\n",
    "                'token_types': {\n",
    "                    'NAME': sum(1 for tok in tokens if tok.type == tokenize.NAME),\n",
    "                    'STRING': sum(1 for tok in tokens if tok.type == tokenize.STRING),\n",
    "                    'NUMBER': sum(1 for tok in tokens if tok.type == tokenize.NUMBER),\n",
    "                    'NEWLINE': sum(1 for tok in tokens if tok.type == tokenize.NEWLINE),\n",
    "                }\n",
    "            }\n",
    "        except tokenize.TokenError:\n",
    "            return None\n",
    "\n",
    "    def evaluate_semantic_similarity(self, original_code: str, back_translated_code: str) -> dict:\n",
    "        \"\"\"Evaluate semantic similarity of code components\"\"\"\n",
    "        # Extract components\n",
    "        orig_comments = self._extract_comments(original_code)\n",
    "        trans_comments = self._extract_comments(back_translated_code)\n",
    "        \n",
    "        orig_functions = self._extract_function_names(original_code)\n",
    "        trans_functions = self._extract_function_names(back_translated_code)\n",
    "        \n",
    "        orig_strings = self._extract_string_literals(original_code)\n",
    "        trans_strings = self._extract_string_literals(back_translated_code)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        comment_similarity = self._calculate_text_similarity(orig_comments, trans_comments)\n",
    "        function_similarity = self._calculate_text_similarity(orig_functions, trans_functions)\n",
    "        string_similarity = self._calculate_text_similarity(orig_strings, trans_strings)\n",
    "        \n",
    "        # Calculate overall semantic score\n",
    "        semantic_scores = [score for score in [\n",
    "            comment_similarity.get('similarity') if comment_similarity else None,\n",
    "            function_similarity.get('similarity') if function_similarity else None,\n",
    "            string_similarity.get('similarity') if string_similarity else None\n",
    "        ] if score is not None]\n",
    "        \n",
    "        overall_semantic_score = np.mean(semantic_scores) if semantic_scores else 0.0\n",
    "        \n",
    "        return {\n",
    "            'comments': comment_similarity,\n",
    "            'functions': function_similarity,\n",
    "            'strings': string_similarity,\n",
    "            'overall_semantic_score': overall_semantic_score\n",
    "        }\n",
    "\n",
    "    def _extract_comments(self, code: str) -> list:\n",
    "        \"\"\"Extract comments from code\"\"\"\n",
    "        comments = []\n",
    "        for line in code.split('\\n'):\n",
    "            if '#' in line:\n",
    "                comment = line[line.index('#')+1:].strip()\n",
    "                if comment:\n",
    "                    comments.append(comment)\n",
    "        return comments\n",
    "\n",
    "    def _extract_function_names(self, code: str) -> list:\n",
    "        \"\"\"Extract function names from code\"\"\"\n",
    "        function_names = []\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.FunctionDef):\n",
    "                    function_names.append(node.name)\n",
    "        except:\n",
    "            pass\n",
    "        return function_names\n",
    "\n",
    "    def _extract_string_literals(self, code: str) -> list:\n",
    "        \"\"\"Extract string literals from code\"\"\"\n",
    "        return re.findall(r'\"([^\"]*)\"', code) + re.findall(r\"'([^']*)'\", code)\n",
    "\n",
    "    def _calculate_text_similarity(self, texts1: list, texts2: list) -> dict:\n",
    "        \"\"\"Calculate semantic similarity between two lists of texts\"\"\"\n",
    "        if not texts1 or not texts2:\n",
    "            return None\n",
    "            \n",
    "        # Get embeddings for all texts\n",
    "        embeddings1 = self.semantic_model.encode(texts1)\n",
    "        embeddings2 = self.semantic_model.encode(texts2)\n",
    "        \n",
    "        # Calculate average similarity\n",
    "        similarities = []\n",
    "        for emb1, emb2 in zip(embeddings1, embeddings2):\n",
    "            similarity = 1 - cosine(emb1, emb2)\n",
    "            similarities.append(similarity)\n",
    "        \n",
    "        return {\n",
    "            'similarity': np.mean(similarities),\n",
    "            'individual_scores': list(zip(texts1, texts2, similarities))\n",
    "        }\n",
    "\n",
    "def evaluate_translations(input_file: str, output_file: str):\n",
    "    \"\"\"Evaluate translations from a file and save results\"\"\"\n",
    "    # Initialize evaluator\n",
    "    evaluator = CodeTranslationEvaluator()\n",
    "    \n",
    "    # Read translations\n",
    "    df = pd.read_csv(input_file)\n",
    "    results = []\n",
    "    \n",
    "    # Evaluate each translation\n",
    "    for _, row in df.iterrows():\n",
    "        evaluation = evaluator.evaluate_translation(\n",
    "            row['original'],\n",
    "            row['back_translated']\n",
    "        )\n",
    "        \n",
    "        # Flatten the nested dictionary for DataFrame storage\n",
    "        flattened_result = {\n",
    "            'original': row['original'],\n",
    "            'hind_translated': row['hindi'],\n",
    "            'back_translated': row['back_translated'],\n",
    "            'bleu_score': evaluation['bleu_score'],\n",
    "            'syntax_valid': evaluation['syntax_validation']['is_valid'],\n",
    "            'syntax_error': evaluation['syntax_validation']['error'],\n",
    "            'semantic_score': evaluation['semantic_similarity']['overall_semantic_score'],\n",
    "            'overall_score': evaluation['overall_score']\n",
    "        }\n",
    "        results.append(flattened_result)\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\nEvaluation Summary for {input_file}:\")\n",
    "    print(f\"Average BLEU Score: {results_df['bleu_score'].mean():.4f}\")\n",
    "    print(f\"Syntax Valid Rate: {results_df['syntax_valid'].mean():.4f}\")\n",
    "    print(f\"Average Semantic Similarity: {results_df['semantic_score'].mean():.4f}\")\n",
    "    print(f\"Overall Score: {results_df['overall_score'].mean():.4f}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    evaluation_files = [\n",
    "        'data/backtranslation_results_10r_5e.csv',\n",
    "        'data/backtranslation_results_10r_10e.csv',\n",
    "        'data/backtranslation_results_10r_20e.csv',\n",
    "        'data/backtranslation_results_10r_30e.csv'\n",
    "    ]\n",
    "    \n",
    "    output_files = [\n",
    "        'data/comprehensive_evaluation_5e.csv',\n",
    "        'data/comprehensive_evaluation_10e.csv',\n",
    "        'data/comprehensive_evaluation_20e.csv',\n",
    "        'data/comprehensive_evaluation_30e.csv'\n",
    "    ]\n",
    "    \n",
    "    for input_file, output_file in zip(evaluation_files, output_files):\n",
    "        evaluate_translations(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading evaluation data...\n",
      "Generating visualizations...\n",
      "All visualizations have been generated in 'evaluation_graphs' directory\n",
      "Loading evaluation data...\n",
      "Generating visualizations...\n",
      "All visualizations have been generated in 'evaluation_graphs' directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "class EvaluationVisualizer:\n",
    "    def __init__(self):\n",
    "        Path('evaluation_graphs').mkdir(exist_ok=True)\n",
    "        sns.set_palette(\"husl\")\n",
    "\n",
    "    def load_evaluation_data(self):\n",
    "        \"\"\"Load evaluation results for all configurations\"\"\"\n",
    "        configs = ['5e', '10e', '20e', '30e']\n",
    "        data = {}\n",
    "        for config in configs:\n",
    "            try:\n",
    "                df = pd.read_csv(f'data/comprehensive_evaluation_{config}.csv')\n",
    "                data[f'{config[:-1]} Examples'] = df\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: Could not find data file for {config} configuration\")\n",
    "        return data\n",
    "\n",
    "    def plot_overall_metrics_comparison(self, data):\n",
    "        \"\"\"Create bar plot comparing metrics across configurations\"\"\"\n",
    "        metrics = {\n",
    "            'Configuration': [],\n",
    "            'BLEU Score': [],\n",
    "            'Syntax Valid Rate': [],\n",
    "            'Semantic Score': [],\n",
    "            'Overall Score': []\n",
    "        }\n",
    "        \n",
    "        for config, df in data.items():\n",
    "            metrics['Configuration'].append(config)\n",
    "            metrics['BLEU Score'].append(df['bleu_score'].mean())\n",
    "            metrics['Syntax Valid Rate'].append(df['syntax_valid'].mean())\n",
    "            metrics['Semantic Score'].append(df['semantic_score'].mean())\n",
    "            metrics['Overall Score'].append(df['overall_score'].mean())\n",
    "        \n",
    "        metrics_df = pd.DataFrame(metrics)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = metrics_df.set_index('Configuration').plot(kind='bar', width=0.8)\n",
    "        plt.title('Comparison of Evaluation Metrics Across Configurations')\n",
    "        plt.xlabel('Number of Examples')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt='%.3f')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('evaluation_graphs/overall_metrics_comparison.png')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_score_distributions(self, data):\n",
    "        \"\"\"Create violin plots showing score distributions\"\"\"\n",
    "        # Prepare data for plotting\n",
    "        plot_data = []\n",
    "        \n",
    "        for config, df in data.items():\n",
    "            for score_type in ['bleu_score', 'semantic_score', 'overall_score']:\n",
    "                score_name = score_type.replace('_', ' ').title()\n",
    "                scores = df[score_type].tolist()\n",
    "                plot_data.extend([{\n",
    "                    'Configuration': config,\n",
    "                    'Score Type': score_name,\n",
    "                    'Score': score\n",
    "                } for score in scores])\n",
    "        \n",
    "        plot_df = pd.DataFrame(plot_data)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.violinplot(data=plot_df, x='Configuration', y='Score', hue='Score Type')\n",
    "        plt.title('Score Distributions Across Configurations')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('evaluation_graphs/score_distributions.png')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_syntax_analysis(self, data):\n",
    "        \"\"\"Create visualization for syntax validation results\"\"\"\n",
    "        syntax_data = {\n",
    "            'Configuration': [],\n",
    "            'Valid Syntax': [],\n",
    "            'Invalid Syntax': []\n",
    "        }\n",
    "        \n",
    "        for config, df in data.items():\n",
    "            valid_rate = df['syntax_valid'].mean()\n",
    "            syntax_data['Configuration'].append(config)\n",
    "            syntax_data['Valid Syntax'].append(valid_rate)\n",
    "            syntax_data['Invalid Syntax'].append(1 - valid_rate)\n",
    "        \n",
    "        syntax_df = pd.DataFrame(syntax_data)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = syntax_df.set_index('Configuration').plot(kind='bar', stacked=True)\n",
    "        plt.title('Syntax Validation Results')\n",
    "        plt.xlabel('Number of Examples')\n",
    "        plt.ylabel('Proportion')\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt='%.1f%%', label_type='center')\n",
    "            \n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('evaluation_graphs/syntax_analysis.png')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_semantic_similarity_heatmap(self, data):\n",
    "        \"\"\"Create heatmap for semantic similarity scores\"\"\"\n",
    "        semantic_data = []\n",
    "        \n",
    "        for config, df in data.items():\n",
    "            semantic_data.append({\n",
    "                'Configuration': config,\n",
    "                'Mean Score': df['semantic_score'].mean(),\n",
    "                'Min Score': df['semantic_score'].min(),\n",
    "                'Max Score': df['semantic_score'].max(),\n",
    "                'Std Dev': df['semantic_score'].std()\n",
    "            })\n",
    "        \n",
    "        semantic_df = pd.DataFrame(semantic_data).set_index('Configuration')\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(semantic_df, annot=True, cmap='YlOrRd', fmt='.3f',\n",
    "                   cbar_kws={'label': 'Score'})\n",
    "        plt.title('Semantic Similarity Analysis')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('evaluation_graphs/semantic_similarity_heatmap.png')\n",
    "        plt.close()\n",
    "\n",
    "    def generate_all_visualizations(self):\n",
    "        \"\"\"Generate all visualization graphs\"\"\"\n",
    "        print(\"Loading evaluation data...\")\n",
    "        data = self.load_evaluation_data()\n",
    "        \n",
    "        if not data:\n",
    "            print(\"No evaluation data found!\")\n",
    "            return\n",
    "            \n",
    "        print(\"Generating visualizations...\")\n",
    "        self.plot_overall_metrics_comparison(data)\n",
    "        self.plot_score_distributions(data)\n",
    "        self.plot_syntax_analysis(data)\n",
    "        self.plot_semantic_similarity_heatmap(data)\n",
    "        print(\"All visualizations have been generated in 'evaluation_graphs' directory\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    visualizer = EvaluationVisualizer()\n",
    "    visualizer.generate_all_visualizations()\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    visualizer = EvaluationVisualizer()\n",
    "    visualizer.generate_all_visualizations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylinguist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
